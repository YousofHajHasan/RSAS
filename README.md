
# Real-time Security & Analytics System (RSAS)  
  
## Description  
RSAS is a fully integrated system to secure places and create logs using *CCTV* cameras and Python in real-time. All of the used data examples were generated by my own house *CCTV* cameras.  

## Table of contents
The following are describing the workflow of this project:  
- [General Structure (Classes and Files)](#general-structure)
	- [Files Overview](#Files-Overview)
	- [Requirements and Dependencies](#Requirements-and-Dependencies)
- [RTSP Connection](#RTSP-Connection)
- [Object Detection](#Object-Detection)  
- [Object Tracking](#Object-Tracking)
- [Analyzing Videos](#Analyzing-Videos)
	- [Tracking cars movement](#Tracking-cars-movement)
	- [Determine in/out the house](#Determine-in/out-the-house)
	- [Checking if the car at the house or not](#Checking-if-the-car-at-the-house-or-not)
	- [Number of people inside the house at the moment](#Number-of-people-inside-the-house-at-the-moment)
- [Multiprocessing](#Multiprocessing)
- [Notification Sending](#Notification-Sending)
	- [Monitoring](#Monitoring)
	- [Sending](#Sending)
- [Testing](#Testing)
	- [Skipping Mechanism](#Skipping-Mechanism)
- [What's Next !!](#What's-Next)
  
## Sample Output

<p align="center">
  <img src="https://github.com/YousofHajHasan/RSAS/blob/main/gifs/DemoVid.gif" />
</p>
  
## General Structure <a id="general-structure"></a>
  
### Files Overview  <a id="Files-Overview"></a>
- `classes.py` This file contains the `Camera` class, which is considered the core element in this project; it contains multiple functions used in the project.  
You can find all of the details for each function as a description at the start of each function.  
  
- `main.py` This file contains the `multiprocessing` mechanism of the project, such as initializing camera objects, creating a manager for multiprocessing, and finally calling the `detect` function.  
Feel free to create a script to try the other `classes.py` functions.  
  
- `sort.py`: This script is a modified version of the original from the `sort` repository by [abewley](https://github.com/abewley). Modifications were made to adapt it to the specific requirements of my project. The original code is distributed under the GPL-3.0 license, and my modifications are in compliance with this license.  
  
### Requirements and Dependencies  <a id="Requirements-and-Dependencies"></a>
To run this project, you need the following:  
- Installing Python libraries such as: `opencv-python`, `numpy`, `keyboard`, `ultralytics`, `requests`.  
**You can easily do that by running the following command:**  
```  
pip install requirements.txt  
```

## RTSP Connection <a id="RTSP-Connection"></a>

My cameras are connected to a DVR by HIKVision, which can create **RTSP** streams. Each camera broadcasts a live stream with 15 FPS.

To connect to HIKVision DVRs using Python, you need the following details:
1. Login Credentials for the device. (Username, Password)
2. IP address and port of the device. (The port is usually **554**)
3. Ensure you are accessing them from the same network the cameras are connected to.
4. Finally, combine these in an RTSP URL as follows:
```python
"rtsp://Username:Password@IP:Port/Streaming/Channels/{CameraNumber}01"
```
After having this URL, you can use the `opencv` library to read it as any standard video stream and then preview it.

* If you can't connect to the same network, you must create port forwarding to access the cameras. **This could be risky**

## Object Detection <a id="Object-Detection"></a>

For this version, I've trained a **YOLOv8** on a custom dataset that I've generated from my different cameras, which can detect two classes: Persons and Cars.
For now, the model type was the (**s**) model because of its size so that I can detect objects with acceptable accuracy with real-time processing based on my machine's specs.

Since each camera has a different angle and will detect shapes of various sizes and appearances among the other cameras, I think one of the improvements for the case of **Multiprocessing** with the **Object Detection** model is to create a specific Smaller AI model for each camera, so, I can have better results for each camera with faster speed. This modification needs more work on preparing the dataset and in the training phase.

**The model works well in the morning and evening, even when the lighting changes.*

## Object Tracking <a id="Object-Tracking"></a>
As mentioned earlier, I have used **SORT** (Simple Online and Realtime Tracking) algorithm to track the detected objects.

- Why did I use it? 

Since I'm aiming for real-time processing on multiple cameras, I need a lightweight algorithms to achieve that. 

- What are the changes that I have made to the algorithm's code?

I've added new attributes to the `KalmanBoxTracker` class.
I've changed the form of the input and the output of the `update` function inside the `Sort` class.

- What could be improved for tracking?

Sure, a more complex algorithm could be used. However, one thing I want to mention is adding a feature to the tracking algorithm that handles instances when the detector fails to identify a bounding box for a known object. This feature would enable the algorithm to suggest a bounding box based on historical data, followed by a similarity check between the box predicted by the tracking algorithm and the object’s appearance in the most recent previous frame.

## Analyzing Videos <a id="Analyzing-Videos"></a>
There are a lot of features that can be extracted from the videos.

These features will be previewed next

--- 
#### Tracking cars movement (Right to Left / Left to Right) <a id="Tracking-cars-movement"></a>
 
<p align="center">
  <img src="https://github.com/YousofHajHasan/RSAS/blob/main/gifs/gif1.gif" />
</p>

The idea of ​​this feature is to count the number of cars moving between two areas and draw a box based on the area the vehicle is currently in.
(**Left** area as **Blue**) (**Right** area as **Red**).

The calculations behind this feature are calculating the cross-product between two vectors with the same origin.

- Vector1 represents the red drawn line in the frame --> (A, B) 
- Vector2 represents the line between **A** and the middle of the car's box --> (A, C)

Based on the output sign, we can determine the area that this car is actually in.

The following picture explains it:

<p align="center">
  <img src="https://github.com/YousofHajHasan/RSAS/blob/main/gifs/CrossProductExplained.png" width="50%" />
</p>


After recording this information, the data can be used to compare different times of the day, traffic, or any other related analysis. It can also be done for people, not only cars, but with adjustments to the code.

---
#### Determine in/out the house <a id="Determine-in/out-the-house"></a>

<p align="center">
  <img src="https://github.com/YousofHajHasan/RSAS/blob/main/gifs/gif2.gif" width="70%" />
</p>

This feature is considered one of the security features that determines a person's position, whether inside or outside the house. (**Inside** as **Red**) (**Outside** as **Blue**).

Basically, there are four predefined points in the second camera's frame, and these points form a polygon. A value will be returned after passing the points with the bottom right of the person's box to the `pointpolygontest` function, which is based on the **Ray Casting** algorithm. Also, based on its sign, we can determine if the person is inside or outside the defined points.

---
#### Checking if the car at the house or not <a id="Checking-if-the-car-at-the-house-or-not"></a>

This simple feature was applied to two cameras; it just checks if the car's status if is at the house or not, and if the status changes, it records the time the vehicle left or arrived at the house. 

---
#### Number of people inside the house at the moment <a id="Number-of-people-inside-the-house-at-the-moment"></a>

This feature is based on the `Determine in/out The House` feature, but it needs a correct initialization at the start of the program to avoid any logical errors.

* For initialization, check the value of `shared_data["Right Now Inside"]` in `main.py`

## Multiprocessing <a id="Multiprocessing"></a>

In my RSAS system, multiprocessing plays a pivotal role, especially considering the real-time aspect of the project. Due to the need to gather real-time information from all cameras, I decided to implement asynchronous processing. This setup allows each camera to independently initialize its data, models, and results. Afterwards, the `multiprocessing.Manager()` combines all these individual outcomes into one shared dictionary, ensuring seamless and simultaneous processing across multiple streams.

The shared data structure facilitated by the manager is crucial for synchronizing states and results across different processes. It helps maintain a consistent and updated view of the system's state, which is essential for real-time analytics and decision-making. This approach ensures that my system can handle multiple streams without sacrificing speed or performance, effectively managing the complexity of concurrent data processing.

## Notification Sending <a id="Notification-Sending"></a>

The notification process is broken down into two critical steps: Monitoring and Sending. 

#### Monitoring <a id="Monitoring"></a>
The Monitoring step involves constantly scanning the video feeds for predefined triggers, such as unauthorized access or unusual activities. This process is primarily handled by the `monitor_shared_data` function, which actively checks the shared data repository for any changes that match the alert criteria. The function continuously evaluates the data collected from all camera feeds, detecting anomalies or events that require immediate attention. Once a relevant event is detected, the system logs the event and prepares the necessary information for the next step in the notification process.

Sample of the resulting JSON file:

<p align="center">
  <img src="https://github.com/YousofHajHasan/RSAS/blob/main/gifs/JSONexample.PNG" width="30%" />
</p>

---
#### Sending <a id="Sending"></a>

After an event has been identified and logged, the Sending step takes over. This step dispatches notifications to the appropriate recipients using the `send_push_notification` function. This function integrates with the Pushover API to send out real-time alerts. It constructs the message payload with the necessary details, such as the user's token and message content, and then posts this data to the Pushover service.

Here's a sample of the output:
<p align="center">
  <img src="https://github.com/YousofHajHasan/RSAS/blob/main/gifs/Noti.jpg" width="30%" />
</p>

## Testing <a id="Testing"></a>
Any testing was done locally on my pc, which is AMD Ryzen 5 3600 6-core processor, with RTX 2070 and 32 GB RAM.

- **4 Cameras** (The most important ones)

This case was examined for nearly two hours, resulting in zero delay. 

CPU usage: an average of 45%
GPU usage: an average of 70%
RAM usage: an average of 40%

Example of resource consumption

<p align="center">
  <img src="https://github.com/YousofHajHasan/RSAS/blob/main/gifs/4 cameras.png" width="30%" />
</p>

- **7 Cameras** (All of them)


This test case has led to a massive increase in CPU usage with similar GPU usage values. 

CPU usage: an average of 90%
GPU usage: an average of 75%
RAM usage: an average of 48%

Example of resource consumption

<p align="center">
  <img src="https://github.com/YousofHajHasan/RSAS/blob/main/gifs/7 cameras.png" width="30%" />
</p>

Because of the high load on the CPU, a delay occurred in some cameras, so I had to come up with a temporary solution for that, which is frame skipping.

#### Skipping Mechanism <a id="Skipping-Mechanism"></a>

The skipping mechanism in this function ensures smooth real-time performance by skipping frames when processing takes too long. Here's a breakdown:

1.  **Target Frame Rate:**
    -   The function aims to process frames at 15 FPS, meaning each frame should ideally take about 0.0667 seconds.


2.  **Elapsed and Threshold Time Calculation:**
    -   For each frame, the function calculates the target time for the next frame and a threshold time to monitor delays. The threshold ensures the system doesn’t fall too far behind.


3.  **Skipping Condition:**
    -   If the elapsed time exceeds the calculated threshold, the **skip flag** is activated. This indicates that the system is falling behind the target frame rate.


4.  **Skipping Logic:**
    -   On the next loop iteration, if the skip flag is set, the function skips the current frame and immediately moves to the next one without processing it. The skip flag is then reset. This means, in the worst case, 6-7 frames will be processed for each second.


5.  **Impact of Skipping:**
    -   Skipping prevents the system from getting stuck trying to process every frame, maintaining responsiveness. Although some frames may be dropped, this ensures the system stays as close as possible to real-time performance.

6. **Drawbacks:**
	- Even if it's a good approach, if the load is too heavy, the 6-7 frames skipping is not enough; thus, more intelligent mechanisms, such as skipping a higher number of frames or resetting the camera to the current frame, are needed.
**Considering that hardware upgrades are not an option.*

## What's Next  !!<a id="What's-Next"></a>

There are a lot of improvements that I can think of to add to the project, but for now, I want to consider this as the first version and give a glimpse of what I can come up with using CCTV cameras and the features that can be added based on needs.

Here are some improvements I think of:

- Adding a user-interface to deal with the cameras.
- As mentioned earlier, a better, more specifically trained model for each camera is needed.
- Some improvements in the tracking algorithms to avoid any object loss.
- Adding a recognition instead of simple tracking so that people can be recognized by their appearance, and the same thing for cars, by identifying the car's plate.
- Adding recognition will lead to considerable security and monitoring system modifications. 
- Adding more features, such as identifying if someone is waiting on the door and awaiting a response.
- Integrating the real-time extracted data with a chatbot that can interact with user's questions and answer them based on available data. 
- Finally, Integrating the system with the cloud to work 24/7 and be an absolute security and analysis system.

### I would be delighted to receive feedback on the project or any suggestions for improvements and new features.
