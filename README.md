
# Real-time Security & Analytics System (RSAS)  
  
## Description  
RSAS is a fully integrated system to secure places and create logs using *CCTV* cameras and Python in real-time. All of the used data examples were generated by my own house *CCTV* cameras.  
  
The following are describing the workflow of this project:  
- General Structure (Classes and Files)  
- RTSP Connection  
- Object Detection  
- Object Tracking  
- Analyzing Videos
	- Tracking cars movement
	- Determine in/out the house
	- Checking if the car at the house or not
	- Number of people inside the house at the moment
- Multiprocessing  
- Notification Sending  
	- Monitoring
	- Sending
- Testing  
- What's Next !!  
  
## Sample Output  
  
## General Structure  
  
### Files Overview  
- `classes.py` This file contains the `Camera` class, which is considered the core element in this project; it contains multiple functions used in the project.  
You can find all of the details for each function as a description at the start of each function.  
  
- `main.py` This file contains the `multiprocessing` mechanism of the project, such as initializing camera objects, creating a manager for multiprocessing, and finally calling the `detect` function.  
Feel free to create a script to try the other `classes.py` functions.  
  
- `sort.py`: This script is a modified version of the original from the `sort` repository by [abewley](https://github.com/abewley). Modifications were made to adapt it to the specific requirements of my project. The original code is distributed under the GPL-3.0 license, and my modifications are in compliance with this license.  
  
### Requirements and Dependencies  
To run this project, you need the following:  
- Installing Python libraries such as: `opencv-python`, `numpy`, `keyboard`, `ultralytics`, `requests`.  
**You can easily do that by running the following command:**  
```  
pip install requirements.txt  
```

## RTSP Connection

My cameras are connected to a DVR by HIKVision, which can create **RTSP** streams.

To connect to HIKVision DVRs using Python, you need the following details:
1. Login Credentials for the device. (Username, Password)
2. IP address and port of the device. (The port is usually **554**)
3. Ensure you are accessing them from the same network the cameras are connected to.
4. Finally, combine these in an RTSP URL as follows:
```python
"rtsp://Username:Password@IP:Port/Streaming/Channels/{CameraNumber}01"
```
After having this URL, you can use the `opencv` library to read it as any standard video stream and then preview it.

* If you can't connect to the same network, you must create port forwarding to access the cameras. **This could be risky**

## Object Detection

For this version, I've trained a **YOLOv8** on a custom dataset that I've generated from my different cameras, which can detect two classes: Persons and Cars.
For now, the model type was the (**s**) model because of its size so that I can detect objects with acceptable accuracy with real-time processing based on my machine's specs.

Since each camera has a different angle and will detect shapes of various sizes and appearances among the other cameras, I think one of the improvements for the case of **Multiprocessing** with the **Object Detection** model is to create a specific Smaller AI model for each camera, so, I can have better results for each camera with faster speed. This modification needs more work on preparing the dataset and in the training phase.

## Object Tracking
As mentioned earlier, I have used **SORT** (Simple Online and Realtime Tracking) algorithm to track the detected objects.

- Why did I use it? 

Since I'm aiming for real-time processing on multiple cameras, I need a lightweight algorithms to achieve that. 

- What are the changes that I have made to the algorithm's code?

I've added new attributes to the `KalmanBoxTracker` class.
I've changed the form of the input and the output of the `update` function inside the `Sort` class.

- What could be improved for tracking?

Sure, a more complex algorithm could be used. However, one thing I want to mention is adding a feature to the tracking algorithm that handles instances when the detector fails to identify a bounding box for a known object. This feature would enable the algorithm to suggest a bounding box based on historical data, followed by a similarity check between the box predicted by the tracking algorithm and the object’s appearance in the most recent previous frame.

## Analyzing Videos
There are a lot of features that can be extracted from the videos.

These features will be previewed next

--- 
#### Tracking cars movement (Right to Left / Left to Right)
 
![](https://github.com/YousofHajHasan/RSAS/blob/main/gifs/gif1.gif)

The idea of ​​this feature is to count the number of cars moving between two areas and draw a box based on the area the vehicle is currently in.
(**Left** area as **Blue**) (**Right** area as **Red**).

The calculations behind this feature are calculating the cross-product between two vectors with the same origin.

- Vector1 represents the red drawn line in the frame --> (A, B) 
- Vector2 represents the line between **A** and the middle of the car's box --> (A, C)

Based on the output sign, we can determine the area that this car is actually in.

The following picture explains it:

![](https://raw.githubusercontent.com/YousofHajHasan/RSAS/main/gifs/CrossProduct.png)

After recording this information, the data can be used to compare different times of the day, traffic, or any other related analysis. It can also be done for people, not only cars, but with adjustments to the code.

---
#### Determine in/out the house

![](https://github.com/YousofHajHasan/RSAS/blob/main/gifs/gif2.gif)

This feature is considered one of the security features that determines a person's position, whether inside or outside the house. (**Inside** as **Red**) (**Outside** as **Blue**).

Basically, there are four predefined points in the second camera's frame, and these points form a polygon. A value will be returned after passing the points with the bottom right of the person's box to the `pointpolygontest` function, which is based on the **Ray Casting** algorithm. Also, based on its sign, we can determine if the person is inside or outside the defined points.

---
#### Checking if the car at the house or not

This simple feature was applied to two cameras; it just checks if the car's status if is at the house or not, and if the status changes, it records the time the vehicle left or arrived at the house. 

---
#### Number of people inside the house at the moment

This feature is based on the `Determine in/out The House` feature, but it needs a correct initialization at the start of the program to avoid any logical errors.

* For initialization, check the value of `shared_data["Right Now Inside"]` in `main.py`

## Multiprocessing

In my RSAS system, multiprocessing plays a pivotal role, especially considering the real-time aspect of the project. Due to the need to gather real-time information from all cameras, I decided to implement asynchronous processing. This setup allows each camera to independently initialize its data, models, and results. Afterwards, the `multiprocessing.Manager()` combines all these individual outcomes into one shared dictionary, ensuring seamless and simultaneous processing across multiple streams.

The shared data structure facilitated by the manager is crucial for synchronizing states and results across different processes. It helps maintain a consistent and updated view of the system's state, which is essential for real-time analytics and decision-making. This approach ensures that my system can handle multiple streams without sacrificing speed or performance, effectively managing the complexity of concurrent data processing.

## Notification Sending

The notification process is broken down into two critical steps: Monitoring and Sending. 

#### Monitoring
The Monitoring step involves constantly scanning the video feeds for predefined triggers, such as unauthorized access or unusual activities. This process is primarily handled by the `monitor_shared_data` function, which actively checks the shared data repository for any changes that match the alert criteria. The function continuously evaluates the data collected from all camera feeds, detecting anomalies or events that require immediate attention. Once a relevant event is detected, the system logs the event and prepares the necessary information for the next step in the notification process.

#### Sending 

After an event has been identified and logged, the Sending step takes over. This step dispatches notifications to the appropriate recipients using the `send_push_notification` function. This function integrates with the Pushover API to send out real-time alerts. It constructs the message payload with the necessary details, such as the user's token and message content, and then posts this data to the Pushover service.

Here's a sample of the output:
![](https://github.com/YousofHajHasan/RSAS/tree/main/gifs/Noti.jpg)

## Testing

## What's Next


